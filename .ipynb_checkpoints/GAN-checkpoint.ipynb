{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Main TFGAN library.\n",
    "tfgan = tf.contrib.gan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Shortcuts for later.\n",
    "slim = tf.contrib.slim\n",
    "layers = tf.contrib.layers\n",
    "ds = tf.contrib.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common functions\n",
    "leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "\n",
    "def evaluate_tfgan_loss(gan_loss, name=None):\n",
    "    \"\"\"Evaluate GAN losses. Used to check that the graph is correct.\n",
    "    \n",
    "    Args:\n",
    "        gan_loss: A GANLoss tuple.\n",
    "        name: Optional. If present, append to debug output.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            gen_loss_np = sess.run(gan_loss.generator_loss)\n",
    "            dis_loss_np = sess.run(gan_loss.discriminator_loss)\n",
    "    if name:\n",
    "        print('%s generator loss: %f' % (name, gen_loss_np))\n",
    "        print('%s discriminator loss: %f'% (name, dis_loss_np))\n",
    "    else:\n",
    "        print('Generator loss: %f' % gen_loss_np)\n",
    "        print('Discriminator loss: %f'% dis_loss_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our input pipeline. Pin it to the CPU so that the GPU can be reserved\n",
    "# for forward and backwards propogation.\n",
    "batch_size = 32\n",
    "with tf.device('/cpu:0'):\n",
    "    with h5py.File(''.join(['bitcoin2012_2017_256_16.h5']), 'r') as hf:\n",
    "        datas = hf['inputs'].value\n",
    "        labels = hf['outputs'].value\n",
    "        #split training validation\n",
    "        training_size = int(0.8* datas.shape[0])\n",
    "        training_datas = datas[:training_size,:]\n",
    "        training_labels = labels[:training_size,:]\n",
    "        validation_datas = datas[training_size:,:]\n",
    "        validation_labels = labels[training_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file_name='bitcoin2012_2017_256_16_GAN'\n",
    "\n",
    "step_size = datas.shape[1]\n",
    "batch_size= 8\n",
    "nb_features = datas.shape[2]\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_fn(noise, weight_decay=2.5e-5):\n",
    "    \"\"\"Simple generator to produce MNIST images.\n",
    "    \n",
    "    Args:\n",
    "        noise: A single Tensor representing noise.\n",
    "        weight_decay: The value of the l2 weight decay.\n",
    "    \n",
    "    Returns:\n",
    "        A generated image in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    with slim.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose, tf.nn.conv1d],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.fully_connected(noise, 1024)\n",
    "        net = layers.fully_connected(net,  7*7 * 256)\n",
    "        net = tf.reshape(net, [-1,  1*7, 256])\n",
    "        net = layers.conv2d_transpose(net, 64, [1, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [1, 4], stride=2)\n",
    "        # Make sure that generator output is in the same range as `inputs`\n",
    "        # ie [-1, 1].\n",
    "        net = tf.nn.conv1d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
    "\n",
    "        return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
